{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375144cc",
   "metadata": {
    "school_cell_uuid": "c878fc07fcc14b6994e915f2c7fe77f5"
   },
   "source": [
    "## 자동 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78da0d9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T06:06:33.409885Z",
     "start_time": "2023-10-06T06:06:27.245206Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b095191",
   "metadata": {
    "school_cell_uuid": "70e9d8ece68040349072fa52c2daea0b"
   },
   "source": [
    "텐서플로의 자동 미분\n",
    "\n",
    "##### `tf.GradientTape`\n",
    "\n",
    "tf.GradientTape는 컨텍스트(context) 안에서 실행된 모든 연산을 테이프(tape)에 \"기록\". \n",
    "\n",
    "그 다음 텐서플로는 후진 방식 자동 미분(reverse mode differentiation)을 사용해 테이프에 \"기록된\" 연산의 그래디언트를 계산합니다.\n",
    "\n",
    "중간 연산 과정(함수, 연산)을 테이프(tape)에 차곡차곡 기록해주는 Gradient tapes 를 제공합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7d24a",
   "metadata": {},
   "source": [
    "* Scalar 를 Scalar로 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbef8c45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T06:19:59.057546Z",
     "start_time": "2023-10-06T06:19:59.043547Z"
    },
    "school_cell_uuid": "73ed72bac16748f79399f90cf7612a60",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape: # tape에 with구문 내의 연산에 관한 모든 것을 저장한다\n",
    "    y = x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a4ada",
   "metadata": {},
   "source": [
    "1. with tf.GradientTape() as tape: 로 저장할 tape을 지정해주면, 이후의 GradientTape() 문맥 아래에서의 TensorFlow의 연관 연산 코드는 tape에 저장이 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d63ed2",
   "metadata": {},
   "source": [
    "2. with ~ as 구문은 enter / exit 의 함수가 내장된 클래스를 사용해야한다.\n",
    "\n",
    "    with 구문 시작시 자동으로 enter함수가 실행되어 as 뒤에 지정한 이름으로 tf.GradientTape() 객체를 복사한다.\n",
    "\n",
    "    with구문이 끝나면 자동으로 exit함수가 발동되며 지정한 이름의 객체를 사용할 수 있게 해주는 방식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe31fbb",
   "metadata": {},
   "source": [
    "\n",
    "3. 이렇게 tape에 저장된 연산 과정 (함수, 연산식) 을 가져다가 TensorFlow는 dx = tape.gradient(loss, x) 로 후진 모드 자동 미분 (Reverse mode automatic differentiation) 방법으로 손실에 대한 x의 미분을 계산합니다. \n",
    "\n",
    "    이렇게 계산한 손실에 대한 x의 미분을 역전파(backpropagation)하여 x의 값을 갱신(update)하는 작업을 반복하므로써 변수 x의 답을 찾아가는 학습을 진행합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d30aa8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T06:19:59.794353Z",
     "start_time": "2023-10-06T06:19:59.779192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=9.0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 현재 y = x의 제곱 (x = 3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b5bb4af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T06:20:00.311190Z",
     "start_time": "2023-10-06T06:20:00.294388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dy/dx = 2x --> 미분식\n",
    "# dy = 2x * dx\n",
    "\n",
    "dy_dx = tape.gradient(y, x) # -> tape에 저장해놓은 연산식으로 미분해라 / y를 x로 미분해라\n",
    "dy_dx.numpy()\n",
    "\n",
    "# -> 다시 실행시키려면 상위 with문을 다시 실행시켜야 결과가 나온다.\n",
    "# 이유는 with 구문이라 tape이 한번 사용하고 소멸되기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff346c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T06:22:34.611734Z",
     "start_time": "2023-10-06T06:22:34.594095Z"
    }
   },
   "source": [
    "- 즉, with ~ as 구문은 메모리를 잘 활용하기 위해 객체를 일회성으로 사용하고 메모리를 반환하기 위해 사용하는 것.\n",
    "\n",
    "- 그래서 tf.GradientTape()를 tape에 담아 한번 사용하고 반환하기 위함이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bcd01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73503bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b1ec0d2",
   "metadata": {},
   "source": [
    "* Scalar를 Vector로 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44f956c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T06:51:12.297145Z",
     "start_time": "2023-10-06T06:51:12.275591Z"
    }
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1., 2., 3.]]\n",
    "\n",
    "# persistent=True 는 tape을 일회성이 아니라 계속 사용할수 있는 변수로서 남겨놓겠다\n",
    "with tf.GradientTape(persistent=True) as tape2: # with 구문내에서 연산해서 tape에 저장\n",
    "    y = x @ w + b\n",
    "    loss = tf.reduce_mean(y**2) # 평균제곱오차(MSE) / 원래 (y_hat - y)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7ba47e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T06:51:12.674163Z",
     "start_time": "2023-10-06T06:51:12.646320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[ 0.43868804, -0.70962256],\n",
       "        [ 0.8773761 , -1.4192451 ],\n",
       "        [ 1.3160641 , -2.1288676 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 0.43868804, -0.70962256], dtype=float32)>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dl_dw, dl_db] = tape2.gradient(loss, [w, b]) # loss를 [w,b]로 미분해라 -> 반환 값 2개\n",
    "\n",
    "[dl_dw, dl_db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3277e28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T06:51:13.662849Z",
     "start_time": "2023-10-06T06:51:13.639553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 0.43868804, -0.70962256], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name으로 지정한 딕셔너리로도 계산이 가능하다.\n",
    "my_vars = {\n",
    "    'w': w,\n",
    "    'b': b\n",
    "}\n",
    "\n",
    "grad = tape2.gradient(loss, my_vars)\n",
    "grad['b']\n",
    "\n",
    "# -> persistent = True 를 했기 때문에 계속 tape 객체를 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ab30d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cde98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fae20d0",
   "metadata": {},
   "source": [
    "- 자동미분 컨트롤 하기! \n",
    "\n",
    "  - 자동미분에서는 `tf.Variable`만 기록 합니다! \n",
    "  - A variable + tensor  는 tensor를 반환 -> sclar취급\n",
    "  - `trainable = False` 조건으로 미분 기록을 제어가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c63776d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T06:49:21.974096Z",
     "start_time": "2023-10-06T06:49:21.925779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = (x0**2) + (x1**2) + (x2**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149a9b9f",
   "metadata": {},
   "source": [
    "- 기본적으로 편미분이라 생각하자.\n",
    "- 미분 기록 불가한 변수에 대한 미분 값들은 None을 반환했다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31603bb",
   "metadata": {},
   "source": [
    "#### 기록되고 있는 variable 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80617825",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T06:50:32.342494Z",
     "start_time": "2023-10-06T06:50:32.306337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'x0:0' shape=() dtype=float32, numpy=3.0>,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tape.watched_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01563d6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T06:51:21.725893Z",
     "start_time": "2023-10-06T06:51:21.704795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n",
       " array([[-2.1501553 ,  0.17153408],\n",
       "        [ 0.7102437 , -0.07733455],\n",
       "        [ 0.38945198, -0.24216251]], dtype=float32)>,\n",
       " <tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tape2.watched_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e8c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "205.312px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
